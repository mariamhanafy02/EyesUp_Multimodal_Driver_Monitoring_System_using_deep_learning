{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow\n",
    "#%pip install matplotlib\n",
    "#%pip install pillow\n",
    "#%pip install scipy\n",
    "#%pip install opencv-python\n",
    "#%pip install seaborn\n",
    "#%pip install librosa\n",
    "#%pip install pyaudio\n",
    "#%pip install wave\n",
    "#%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import cv2\n",
    "import IPython.display as display\n",
    "import PIL.Image\n",
    "import time\n",
    "import datetime as dt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import subprocess\n",
    "import json\n",
    "import datetime\n",
    "import wikipedia\n",
    "import webbrowser\n",
    "import ctypes\n",
    "import requests\n",
    "import pyttsx3\n",
    "import openai\n",
    "from urllib.request import urlopen\n",
    "from joblib import dump, load\n",
    "import urllib.request\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ssl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D ,LSTM ,MaxPool2D , Input , GlobalAveragePooling2D ,AveragePooling2D, Dense , Dropout ,Activation, Flatten , BatchNormalization\n",
    "import time\n",
    "import keyboard \n",
    "import threading\n",
    "import requests\n",
    "import importlib\n",
    "from statistics import mode\n",
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample_rate = 44100  # Adjust the target sample rate as needed\n",
    "interest = 'sports'\n",
    "name = 'Hala'\n",
    "messages = [{\"role\": \"system\", \"content\": \"Hello! I am Ghost, a chatbot designed to keep the driver awake by engaging in conversation. Please keep your responses short to avoid distractions by talking about his interests the user's interest is \"+interest+\". If you wish to end the conversation, just say 'exit' or 'thank you'. the user name is \" + name + \". Let's start by greeting you.\"}]\n",
    "\n",
    "#executor=concurrent.futures.ThreadPoolExecutor(max_workers=10)\n",
    "dist_predicted_class=''\n",
    "drow_predicted_class=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distraction_flag=0\n",
    "drowsiness_flag=0\n",
    "check_flag_drow=0\n",
    "check_flag_dist=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "Distraction_vision_model = tf.keras.models.load_model(\"C:/Users/lenovo/Desktop/Graduation_Project/Distraction/inceptionv3c_model.h5\",compile=False)  # Load the inception distraction model\n",
    "Speaking_voice_model = tf.keras.models.load_model(\"C:/Users/lenovo/Desktop/Graduation_Project/Sound/speaking_lstm_model2.h5\",compile=False)\n",
    "speaking_scalar=load('C:/Users/lenovo/Desktop/Graduation_Project/Sound/speaking_scaler.bin')\n",
    "Drowsiness_vision_model =tf.keras.models.load_model(\"C:/Users/lenovo/Desktop/Graduation_Project/drowsiness/inceptionv3_model_drowsiness.h5\",compile=False)\n",
    "drowsiness_voice_model = tf.keras.models.load_model(\"C:/Users/lenovo/Desktop/Graduation_Project/Sound/drowsiness_sound_model.h5\",compile=False)\n",
    "drowsiness_scaler = load(\"C:/Users/lenovo/Desktop/Graduation_Project/Sound/drowsiness_sound_scaler.save\")\n",
    "Distraction_vision_lstm_model=tf.keras.models.load_model(\"C:/Users/lenovo/Desktop/Graduation_Project/Distraction/inceptionv3_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions for Camera Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distraction_detection(img,class_names):\n",
    "    resized_image=cv2.resize(img, (299, 299))\n",
    "    # Preprocess the image for InceptionV3\n",
    "    preprocessed_image = preprocess_input(resized_image)\n",
    "    # Expand dimensions to create a batch (required for prediction)\n",
    "    input_data = np.expand_dims(preprocessed_image, axis=0)\n",
    "    # Make predictions (if needed)\n",
    "    predictions = Distraction_vision_model.predict(input_data,verbose = 0)\n",
    "    # Get the predicted class\n",
    "    predicted_class_idx = np.argmax(predictions)\n",
    "    predicted_class = class_names[predicted_class_idx]\n",
    "    return resized_image,predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distraction_lstm_detection(frames,class_names):\n",
    "    images=[]\n",
    "    for i in range(0,4):\n",
    "        resized_image=cv2.resize(frames[i], (299, 299))\n",
    "        # Preprocess the image for InceptionV3\n",
    "        preprocessed_image = preprocess_input(resized_image)\n",
    "        # Expand dimensions to create a batch (required for prediction)\n",
    "        images.append(preprocessed_image)\n",
    "\n",
    "    input_data = np.expand_dims(images, axis=0)\n",
    "    # Make predictions (if needed)\n",
    "    predictions = Distraction_vision_lstm_model.predict(input_data,verbose = 0)\n",
    "    # Get the predicted class\n",
    "    predicted_class_idx = np.argmax(predictions)\n",
    "    predicted_class = class_names[predicted_class_idx]\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drowsiness_detection(eye):\n",
    "\n",
    "    # Resize the eye to match the input shape of the model\n",
    "    eye_resized = cv2.resize(eye, (80, 80))\n",
    "    \n",
    "    # Convert the eye to RGB\n",
    "    eye_rgb = cv2.cvtColor(eye_resized, cv2.COLOR_BGR2RGB)\n",
    "    preprocessed_image = preprocess_input(eye_rgb)\n",
    "    \n",
    "    # Expand the dimensions to create a batch of size 1\n",
    "    input_eye = np.expand_dims(preprocessed_image, axis=0)\n",
    "    \n",
    "    # Perform prediction using the model\n",
    "    prediction = Drowsiness_vision_model.predict(input_eye,verbose=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename,audio):\n",
    "    with open(filename , \"wb\") as f:\n",
    "        f.write(audio.get_wav_data())\n",
    "    print(\"Opening wave file....\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Detection with Audio for Speaking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TakeRealTimeAudio():\n",
    "\t# threading.Thread(target=Real_Time_Recording).start()\n",
    "    #Real_Time_Recording()\n",
    "\tfilename = \"C:/Users/lenovo/Desktop/Graduation_Project/sound/speaking_audio.wav\" #the recorded audio will be saved there\n",
    "\tr = sr.Recognizer()\n",
    "\tsource=sr.Microphone()\n",
    "\t#print(source.list_microphone_names())\n",
    "\t#lists =source.list_microphone_names()\n",
    "\t#index = lists.index('Microphone Array (Realtek(R) Audio)')\n",
    " \t#device_index=index\n",
    "\twith sr.Microphone() as source:\n",
    "\t\t\n",
    "\t\tprint(\"Listening...\")\n",
    "        \n",
    "\t\tr.pause_threshold = 1\n",
    "\t\taudio = r.listen(source=source, phrase_time_limit=6)\n",
    "\t\tprint(type(audio))\n",
    "\t\topen_file(filename,audio)\n",
    "\t\t\n",
    "\n",
    "\treturn filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract MFCC features from audio file\n",
    "def extract_mfcc_file(file_path, target_sr=22050, n_mfcc=40):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Fix the length if necessary\n",
    "    mfccs = librosa.util.fix_length(data=mfccs, size=700, axis=1)\n",
    "    return mfccs\n",
    "\n",
    "def detect_speaking_file(model, audio_file_path, target_sr=22050, max_pad_len=700, gain_factor=10000.0):\n",
    "    try:\n",
    "        # Extract MFCC features from the audio file\n",
    "        mfccs = extract_mfcc_file(audio_file_path, target_sr=target_sr)\n",
    "        # scaler = StandardScaler()\n",
    "        scaler=speaking_scalar\n",
    "        # Scale the extracted features using the same scaler\n",
    "        mfccs_scaled = scaler.transform(mfccs.reshape(1, -1)).reshape(mfccs.shape)\n",
    "\n",
    "        # Reshape for model input\n",
    "        input_data = np.expand_dims(mfccs_scaled, axis=0)\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(input_data,verbose=0)\n",
    "\n",
    "        # Interpret the prediction (you may need to adjust this based on your model output)\n",
    "        print(\"Speaking Model in progress...\")\n",
    "        if prediction[0, 0] > 0.5:\n",
    "            return\"Speaking\"\n",
    "        else:\n",
    "            return\"Silent\"\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping detection.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot & Actions & Drowsiness by Voice Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "def extract_text_from_voice():\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # Define parameters for recording\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    CHUNK = 1024\n",
    "    DURATION = 7  # Duration of recording is 7 seconds\n",
    "    filename = \"C:/Users/lenovo/Desktop/Graduation_Project/output.wav\"\n",
    "\n",
    "    # Open PyAudio stream for recording\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print('Listening ...')\n",
    "\n",
    "    # Record the audio with increased volume and normalization\n",
    "    frames = []\n",
    "    gain_factor = 2.0  # Adjust the gain factor as needed\n",
    "    for i in range(0, int(RATE / CHUNK * DURATION)):\n",
    "        data = stream.read(CHUNK)\n",
    "        audio_array = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "        # Apply gain and normalization\n",
    "        scaled_audio = (audio_array * gain_factor).astype(np.int16)\n",
    "        normalized_audio = np.int16(scaled_audio / np.max(np.abs(scaled_audio)) * 32767)\n",
    "\n",
    "        frames.append(normalized_audio.tobytes())\n",
    "\n",
    "    print('Stopped')\n",
    "\n",
    "    # Stop and close the PyAudio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    # Terminate the PyAudio object\n",
    "    p.terminate()\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    wf = wave.open(filename, \"wb\")\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    #print(f\"Recording saved as {filename}\")\n",
    "\n",
    "    # Use SpeechRecognition to recognize the speech\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(filename) as source:\n",
    "        audio_data = r.record(source)\n",
    "\n",
    "    try:\n",
    "        print(\"Recognizing...\")\n",
    "        query = r.recognize_google(audio_data, language='en-in')\n",
    "        print(f\"User said: {query}\\n\")\n",
    "        return query\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand audio\")\n",
    "        return \"None\"\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "        return \"None\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Unable to Recognize your voice.\") \n",
    "        return \"None\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(filename, scaler): \n",
    "  #load the file\n",
    "  data, sampling_rate = librosa.load(filename, offset = 0.5)\n",
    "  #preprocess\n",
    "  n_fft =  int (sampling_rate * 0.025)\n",
    "  mfcc = np.mean(librosa.feature.mfcc(y = data, sr = sampling_rate, n_mfcc = 40, n_fft = n_fft).T, axis=0)\n",
    "  #standardize  \n",
    "  mfcc_standardized = scaler.transform(mfcc.reshape(1, -1)).reshape(1, -1)\n",
    "  return mfcc_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration.py\n",
    "\n",
    "class Configuration:\n",
    "    OPENAI_API_KEY = \"ADD YOUR KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = Configuration.OPENAI_API_KEY\n",
    "engine =pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"OpenAI Version:\", openai.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features_from_audio(audio_data, sr, num_mfcc=40, n_fft=2048, hop_length=512):\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Pad or truncate the features to ensure consistent shape\n",
    "    if mfccs.shape[1] < 200:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, 200 - mfccs.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :200]\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_long_enough(text, min_word_count=1):\n",
    "    return len(text.split()) >= min_word_count\n",
    "\n",
    "def interact_with_ChatGPT(content=messages,drowsy='Notdrowsy',situation=''):\n",
    "    global messages\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    importlib.reload(pyttsx3)\n",
    "    engine = pyttsx3.init()\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[1].id)\n",
    "    engine.say(\"Hello\"+name+situation+\" If you wish to end the conversation, just say 'exit' or 'thank you'.\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        importlib.reload(pyttsx3)\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "        engine.setProperty('voice', voices[1].id)\n",
    "        #engine.say(\"Hello \"+name+\"! I am Ghost, I am here to keep you awake. If you wish to end the conversation, just say 'exit' or 'thank you'.\")\n",
    "        user_text = extract_text_from_voice()\n",
    "        if (user_text=='None'):\n",
    "            messages.append({\"role\": \"system\", \"content\": 'system was not able to detect the user voice. ask the user to speak again'})\n",
    "            engine.say('Sorry we were not able to detect your voice.')\n",
    "        if is_long_enough(user_text):\n",
    "            messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "            \n",
    "            query = user_text.lower()   \n",
    "            \n",
    "            # All the commands said by user will be \n",
    "            # stored here in 'query' and will be\n",
    "            # converted to lower case for easily \n",
    "            # recognition of command\n",
    "            if 'exit' in query or \"thank you\" in user_text:\n",
    "                engine.say(\"Thanks for giving me your time\")\n",
    "                engine.runAndWait()\n",
    "                print(\"Thanks for giving me your time\")\n",
    "                break\n",
    "                \n",
    "                \n",
    "            elif 'wikipedia' in query:\n",
    "                print('hereeee')\n",
    "                engine.say('Searching Wikipedia...')\n",
    "                query = query.replace(\"wikipedia\", \"\")\n",
    "                results = wikipedia.summary(query, sentences = 3)\n",
    "                engine.say(\"According to Wikipedia\")\n",
    "                print(results)\n",
    "                engine.say(results)\n",
    "                messages.append({\"role\": \"system\", \"content\": 'system opened wikipedia ask user if he wants anything else if not user just has to say thank you or exit and ask him to leave the phone and focus on the road'})\n",
    "\n",
    "            elif 'open youtube' in query:\n",
    "                engine.say(\"Here you go to Youtube\\n\")\n",
    "                print(\"hena youtube\")\n",
    "                webbrowser.open(\"https://youtube.com\", new=2)\n",
    "                messages.append({\"role\": \"system\", \"content\": 'system opened youtube ask user if he wants anything else if not user just has to say thank you or exit and ask him to leave the phone and focus on the road'})\n",
    "\n",
    "\n",
    "            elif 'open google' in query:\n",
    "                engine.say(\"Here you go to Google\\n\")\n",
    "                webbrowser.open(\"https://google.com\", new=2)\n",
    "                messages.append({\"role\": \"system\", \"content\": 'system opened google ask user if he wants anything else if not user just has to say thank you or exit and ask him to leave the phone and focus on the road'})\n",
    "\n",
    "\n",
    "            elif 'the time' in query:\n",
    "                strTime = datetime.datetime.now().strftime(\"% H:% M:% S\") \n",
    "                engine.say(f\"Sir, the time is {strTime}\")\n",
    "\n",
    "            if (drowsy == 'Drowsy' and user_text != 'None'):\n",
    "                new_audio_file = \"C:/Users/lenovo/Desktop/Graduation_Project/output.wav\"\n",
    "                preprocessed_data_test = preprocess_test(new_audio_file, drowsiness_scaler)\n",
    "                preprocessed_data_test = preprocessed_data_test.reshape((preprocessed_data_test.shape[0], 1, preprocessed_data_test.shape[1]))\n",
    "                # 3. Make Predictions\n",
    "                predictions_test = drowsiness_voice_model.predict(preprocessed_data_test,verbose=0)\n",
    "                # 4. Postprocess audio file => binary\n",
    "                predicted_class_test = np.argmax(predictions_test)\n",
    "                if predicted_class_test == 0:\n",
    "                    result = 'Normal'\n",
    "                else:\n",
    "                    result = 'Sleepy'\n",
    "                    \n",
    "                if(result == 'Normal'):\n",
    "                    print(\"Awake\")\n",
    "                else:\n",
    "                    print(\"Driver appears drowsy. Taking action...\")\n",
    "                    messages.append({\"role\": \"system\", \"content\": 'system has detected that the user is drowsy from the camera and now we detected that the user voice is drowsy so we will ask the user to talk and try to be awake and talk with him about his interests and tell him his drowsiness situation and if he was distracted'})\n",
    "                # pass\n",
    "                \n",
    "            # Extend the conversation context by sending the entire conversation history\n",
    "            response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "            \n",
    "            reply = response.choices[0].message.content\n",
    "            engine.say(reply)\n",
    "            print(reply)\n",
    "            engine.runAndWait()\n",
    "            \n",
    "            messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            \n",
    "        else:\n",
    "            print(\"Please provide a longer response for a more engaging conversation.\")\n",
    "        \n",
    "        # Introduce a delay between requests to avoid rate limit issues\n",
    "        time.sleep(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Detection with Cameras for drowsiness and distraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_laptop() :\n",
    "#     global drow_predicted_class\n",
    "#     global drowsiness_flag\n",
    "#     global check_flag_drow\n",
    "#     #class_names = [\"Drowsy\",\"Notdrowsy\"]\n",
    "\n",
    "#     # 0 laptop\n",
    "#     cv2.startWindowThread() \n",
    "#     cap = cv2.VideoCapture(2)\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(\"Error: Could not open camera.\")\n",
    "#         exit()\n",
    "    \n",
    "#     # Load Haar cascade for eye detection\n",
    "#     eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "#     #print(eye_cascade)\n",
    "#     # Counter for saving images\n",
    "#     image_counter = 0\n",
    "#     drowsiness_output=[]\n",
    "\n",
    "#     cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "#     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"Error: Could not read frame.\")\n",
    "#             break\n",
    "#         # Convert the frame to grayscale for eye detection\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         #print(gray)  #print images pixels\n",
    "#         # Detect eyes in the grayscale frame\n",
    "#         eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#         #print(eyes)\n",
    "#         # Iterate over detected eyes\n",
    "#         for (ex, ey, ew, eh) in eyes:\n",
    "#             #print(\"gowa eye\")\n",
    "#             # Extract the region of interest (eye)\n",
    "#             eye = frame[ey:ey+eh, ex:ex+ew]\n",
    "#             # Save the detected eye as an image\n",
    "#             #eye_filename = f\"eye_{image_counter}.jpg\"\n",
    "#             #cv2.imwrite(eye_filename, eye)\n",
    "#             #print(f\"Saved {eye_filename}\")\n",
    "            \n",
    "#             # Increment image counter\n",
    "#             image_counter += 1\n",
    "#             prediction=drowsiness_detection(eye) #change it to drowsiness\n",
    "#             # Display prediction on the frame\n",
    "#             #print(prediction)\n",
    "#             if prediction[0][0] < 0.50:\n",
    "#                 predicted_class= \"Drowsy\"\n",
    "#                 #cv2.putText(frame, 'Closed', (ex, ey-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "#             else:\n",
    "#                 predicted_class= \"Notdrowsy\"\n",
    "#                 # cv2.putText(frame, 'Open', (ex, ey-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "#             drowsiness_output.append(predicted_class)\n",
    "#             if len(drowsiness_output)==30:\n",
    "            \n",
    "#                 most_common = mode(drowsiness_output)\n",
    "#                 drow_predicted_class=most_common\n",
    "#                 drowsiness_flag=1\n",
    "#                 print(\"300 frames from drowsiness are taken from drowsiness\")\n",
    "#                 cv2.putText(frame, most_common, (ex, ey-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "#                 # Draw a rectangle around the detected eye\n",
    "#                 cv2.rectangle(frame, (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 2)\n",
    "#             if (check_flag_drow==1):\n",
    "#                 drowsiness_output=[]\n",
    "#                 check_flag_drow=0\n",
    "#             break\n",
    "\n",
    "#         cv2.imshow(\"drowsiness\", frame)\n",
    "#         if cv2.waitKey(1) == 27 :\n",
    "#             break\n",
    "#         #if keyboard.is_pressed(\"s\"):\n",
    "#         #    break\n",
    "\n",
    "\n",
    "#     cap.release() \n",
    "#     cv2.destroyWindow(\"drowsiness\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_laptop() :\n",
    "    global drow_predicted_class\n",
    "    global drowsiness_flag\n",
    "    global check_flag_drow\n",
    "    \n",
    "    cv2.startWindowThread() \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera DROW.\")\n",
    "        exit()\n",
    "\n",
    "    drowsiness_output=[]\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    while True:\n",
    "        #print(\"here\")\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame drowsiness\")\n",
    "            break\n",
    "        #print(\"here1\")\n",
    "        image_input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # load face detection model\n",
    "        mp_face = mp.solutions.face_detection.FaceDetection(\n",
    "            model_selection=1,  # model selection\n",
    "            min_detection_confidence=0.5  # confidence threshold\n",
    "        )\n",
    "        #print(\"mp_face\",mp_face)\n",
    "        results = mp_face.process(image_input)\n",
    "        #print(\"results\",results)\n",
    "        if results.detections is not None:\n",
    "            #print(\"results.detections is not None\")\n",
    "            image_rows, image_cols, _ = frame.shape\n",
    "            mp_face_detection = mp.solutions.face_detection\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "            detection = results.detections[0]\n",
    "\n",
    "            eye_left=mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.LEFT_EYE)\n",
    "            #\n",
    "            eye_right=mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.RIGHT_EYE)\n",
    "\n",
    "\n",
    "\n",
    "            eye_r = _normalized_to_pixel_coordinates(eye_right.x,eye_right.y, image_cols,image_rows)\n",
    "            eye_l = _normalized_to_pixel_coordinates(eye_left.x,eye_left.y, image_cols,image_rows)\n",
    "            \n",
    "            # Define the size of the rectangle to be drawn\n",
    "            # Define the size of the rectangle to be drawn based on the eye size\n",
    "            rectangle_size = int(np.linalg.norm(np.array(eye_r) - np.array(eye_l)) / 2.7)\n",
    "\n",
    "            # Define the rectangles for each eye\n",
    "            rectangles = [(eye_r[0]-rectangle_size, eye_r[1]-rectangle_size, eye_r[0]+rectangle_size, eye_r[1]+rectangle_size),\n",
    "                        (eye_l[0]-rectangle_size, eye_l[1]-rectangle_size, eye_l[0]+rectangle_size, eye_l[1]+rectangle_size)]\n",
    "            #print(\"rectangles\",rectangles)\n",
    "            for rect in rectangles:\n",
    "                x1, y1, x2, y2 = rect\n",
    "                # Extract the region of interest (eye)\n",
    "                eye = frame[y1:y2, x1:x2]\n",
    "                #print(\"eye\",eye)\n",
    "                if eye.shape[0]>0 and eye.shape[1]>0:\n",
    "                    #print(\"da5al al if\")\n",
    "                    # Resize the eye to match the input shape of the model\n",
    "                    prediction=drowsiness_detection(eye) #change it to drowsiness\n",
    "                    #print(\"prediction\",prediction)\n",
    "                    confidence=int(prediction[0][0]*100)  #print in streamlit not the code\n",
    "                    #print(\"confidence\",confidence)\n",
    "                    # Display prediction on the frame\n",
    "                    if prediction[0][0] < 0.60:\n",
    "                        predicted_class= \"Drowsy\"\n",
    "                    else:\n",
    "                        predicted_class= \"Notdrowsy\"\n",
    "                    # Draw a rectangle around the detected eye\n",
    "                    drowsiness_output.append(predicted_class)\n",
    "                    if len(drowsiness_output)==30:\n",
    "                        most_common = mode(drowsiness_output)\n",
    "                        drow_predicted_class=most_common\n",
    "                        drowsiness_flag=1\n",
    "                        print(\"frames from drowsiness are taken from drowsiness\")\n",
    "                        cv2.putText(frame, str(confidence), (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                        # Draw a rectangle around the detected eye\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    if (check_flag_drow==1):\n",
    "                        drowsiness_output=[]\n",
    "                        check_flag_drow=0\n",
    "                    break\n",
    "                else:\n",
    "                    print(eye.shape)\n",
    "                    \n",
    "\n",
    "            \n",
    "\n",
    "            cv2.imshow(\"drowsiness\", frame)\n",
    "            if cv2.waitKey(1) == 27 :\n",
    "                break\n",
    "            #if keyboard.is_pressed(\"s\"):\n",
    "            #    break\n",
    "\n",
    "\n",
    "    cap.release() \n",
    "    cv2.destroyWindow(\"drowsiness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_phone() :\n",
    "    global dist_predicted_class\n",
    "    global distraction_flag\n",
    "    global check_flag_dist\n",
    "    class_names = [\"Drive safe\",\"Text Right\",\"Talk Right\",\"Text Left\",\"Talk Left\",\"Adjust Radio\",\"Drink\",\"Reach Behind\",\"Makeup & Hair\",\"Talk Passenger\"]\n",
    "    # 1 phone\n",
    "    cv2.startWindowThread() \n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera DIST.\")\n",
    "        exit()\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    distracted_output=[]\n",
    "    while True:\n",
    "        #weighted average \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame DIST.\")\n",
    "            break\n",
    "        RBG = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        resized_image,predicted_class=distraction_detection(RBG,class_names)\n",
    "        distracted_output.append(predicted_class)\n",
    "        if len(distracted_output)==30:\n",
    "            most_common=mode(distracted_output)\n",
    "            cv2.putText(resized_image, f\"Class: {most_common}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            dist_predicted_class= most_common\n",
    "            print(\"frames from distraction are taken from distraction\")\n",
    "            distraction_flag=1\n",
    "        if (check_flag_dist==1): #this means that the check is done so restart \n",
    "            distracted_output=[]\n",
    "            check_flag_dist=0\n",
    "            #dist_predicted_class= \"Drink\"\n",
    "        cv2.imshow(\"distraction\", frame)\n",
    "        if cv2.waitKey(1) == ord('s'):\n",
    "            break\n",
    "        #if keyboard.is_pressed(\"s\"):\n",
    "        #    break\n",
    "\n",
    "    cap.release() \n",
    "    cv2.destroyWindow(\"distraction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_phone_lstm() :\n",
    "    global dist_predicted_class\n",
    "    global distraction_flag\n",
    "    global check_flag_dist\n",
    "    class_names = [\"Drive safe\",\"Text Right\",\"Talk Right\",\"Text Left\",\"Talk Left\",\"Adjust Radio\",\"Drink\",\"Reach Behind\",\"Makeup & Hair\",\"Talk Passenger\"]\n",
    "    frames=[]\n",
    "    # 1 phone\n",
    "    cv2.startWindowThread() \n",
    "    cap = cv2.VideoCapture(2)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera DIST.\")\n",
    "        exit()\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    distracted_output=[]\n",
    "    while True:\n",
    "        #weighted average \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame DIST.\")\n",
    "            break\n",
    "        RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(RGB)\n",
    "        if len(frames)==4:\n",
    "            #print(\"4 Frames for one prediction of LSTM\")\n",
    "            predicted_class=distraction_lstm_detection(frames,class_names)\n",
    "            distracted_output.append(predicted_class)\n",
    "            if len(distracted_output)==30:\n",
    "                most_common=mode(distracted_output)\n",
    "                cv2.putText(frame, f\"Class: {most_common}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                dist_predicted_class= most_common\n",
    "                print(\"frames from distraction are taken from distraction\")\n",
    "                distraction_flag=1\n",
    "            if (check_flag_dist==1): #this means that the check is done so restart \n",
    "                distracted_output=[]\n",
    "                check_flag_dist=0\n",
    "                #dist_predicted_class= \"Drink\"\n",
    "            frames=[]\n",
    "            \n",
    "        cv2.imshow(\"distraction\", frame)\n",
    "        if cv2.waitKey(1) == ord('s'):\n",
    "            break\n",
    "        #if keyboard.is_pressed(\"s\"):\n",
    "        #    break\n",
    "\n",
    "    cap.release() \n",
    "    cv2.destroyWindow(\"distraction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading between the 2 Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thread1=threading.Thread(target=detect_phone) # thread for camera phone \n",
    "thread1=threading.Thread(target=detect_phone_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread3=threading.Thread(target=detect_laptop) # thread for camera laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Cases Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_result(): #check both drowsiness and distraction at the same time \n",
    "    global distraction_flag\n",
    "    global drowsiness_flag\n",
    "    global check_flag_drow\n",
    "    global check_flag_dist\n",
    "    while True:\n",
    "        importlib.reload(pyttsx3)\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "        engine.setProperty('voice', voices[1].id)\n",
    "        if (drowsiness_flag==1 and distraction_flag==1):\n",
    "            #print(f\"thread 1 / mobile camera {thread1.is_alive()}\")\n",
    "            #print(f\"thread 3 / laptop camera {thread3.is_alive()}\")\n",
    "\n",
    "            if (thread1.is_alive()==False) or (thread3.is_alive()==False):\n",
    "                \n",
    "                #print(\"hbreak men al checK 3lshan wa7da men cameras esc 5ALST \")\n",
    "                break\n",
    "            #print(\"gowa al if\")\n",
    "            \n",
    "            predicted_Distraction_vision_class_name=dist_predicted_class\n",
    "            predicted_Drowsiness_vision_class_name=drow_predicted_class\n",
    "\n",
    "            #FOR REVIWING\n",
    "            #predicted_Distraction_vision_class_name=\"Talk Passenger\"\n",
    "            #predicted_Drowsiness_vision_class_name=\"Drowsy\"\n",
    "\n",
    "\n",
    "            if (predicted_Distraction_vision_class_name == \"Drive safe\") and (predicted_Drowsiness_vision_class_name == \"Notdrowsy\"):\n",
    "                print(\"Pass because he is driving safely and not drowsy as well\")\n",
    "            \n",
    "            elif (predicted_Distraction_vision_class_name == \"Drive safe\") and (predicted_Drowsiness_vision_class_name == \"Drowsy\"):\n",
    "                print(\"driver is driving safe but he is drowsy so we need chatbot in order to confirm so\")\n",
    "            \n",
    "                print(\"Chatgpt in progress\")\n",
    "                interact_with_ChatGPT(situation = 'You are being Drowsy, Lets have a conversation to keep you awake',drowsy=predicted_Drowsiness_vision_class_name,content=\"ALERT the user: It seems like user is feeling drowsy.keep responses very short around one sentence please. It's essential to stay alert while driving. Consider taking a short break, opening the window for fresh air, or having a caffeinated drink if it's safe to do so. Your safety is our priority.I am Ghost, your chatbot companion. As your driving companion, my primary goal is to ensure your safety and well-being on the road.\")\n",
    "            \n",
    "            elif (predicted_Distraction_vision_class_name in [\"Talk Right\", \"Talk Left\"] and (predicted_Drowsiness_vision_class_name == \"Drowsy\")):\n",
    "                print(\"Chatgpt in progress\")\n",
    "\n",
    "                interact_with_ChatGPT(situation = 'You are being distracted and drowsy. Please focus on the road and be alert', drowsy=predicted_Drowsiness_vision_class_name,content=\"I am Ghost, your chatbot companion.keep responses very short around one sentence please. As your driving companion, my primary goal is to ensure your safety and well-being on the road. Please keep your responses short to avoid distractions. If you ever need assistance, feel free to ask. ALERT: It appears that you might be feeling drowsy and distracted by something else. It's crucial to maintain focus and alertness while driving. Firstly, try to address the distraction by safely pulling over if necessary or eliminating the source of distraction. Once that's addressed, consider taking a short break to rest and refresh yourself. If possible, engage in some simple exercises or stretches to increase alertness. Your safety and the safety of others on the road are our top priorities. You can end the conversation at any time by saying thank you or exit\")\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Talk Right\", \"Talk Left\"] and (predicted_Drowsiness_vision_class_name == \"Notdrowsy\")):\n",
    "                print(\"driver is distracted by Talkling and also drowsy so we need chatbot\")  # to be removed later\n",
    "            \n",
    "                engine.say(\"PLEASE FOCUS ON THE ROAD YOU HAVE BEEN TALKING ON THE PHONRE FOR A WHILE\")\n",
    "                engine.runAndWait()\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Adjust Radio\"] ):\n",
    "                print(\"driver is distracted by Adjust Radio\")  # to be removed later\n",
    "                engine.say(\"PLEASE FOCUS ON THE ROAD YOU HAVE BEEN ADJUSTING RADIO FOR A WHILE\")\n",
    "                engine.runAndWait()\n",
    "\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Drink\"] ):\n",
    "                print(\"driver is distracted by  Drink\")  # to be removed later\n",
    "                engine.say(\"PLEASE FOCUS ON THE ROAD YOU HAVE BEEN DRINKING FOR A WHILE\")\n",
    "                engine.runAndWait()\n",
    "\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Reach Behind\"] ):\n",
    "                print(\"driver is distracted by Reach Behind\")  # to be removed later\n",
    "                engine.say(\"PLEASE FOCUS ON THE ROAD YOU HAVE BEEN REACHING BEHIND FOR A WHILE\")\n",
    "                engine.runAndWait()\n",
    "\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Makeup & Hair\"] ):\n",
    "                print(\"driver is distracted by Makeup & Hair\")  # to be removed later\n",
    "                engine.say(\"PLEASE FOCUS ON THE ROAD YOU ARE HAVE BEEN FIXING YOUR HAIR OR MAKEUP FOR A WHILE\")\n",
    "                engine.runAndWait()\n",
    "\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Text Right\", \"Text Left\"]):\n",
    "                print(\"driver is distracted by texting\")\n",
    "                interact_with_ChatGPT(situation = 'You are using your phone while driving. I can open to you google, youtube or search wikipedia for you is there is anything that I can help you with' ,content = 'User is Texting while driving  and you are ghost a chatbot. keep responses very short around one sentence please to help the user stay focused while driving so ask the user to stay focused and if he want to open google or youtube or search wikipedia and never say that you can not open youtube as our system can and will open it so that the user can focus more ron the road if the user does not need anything tell him to tell you thank you or exit')\n",
    "            elif (predicted_Distraction_vision_class_name in [\"Talk Passenger\"]) :\n",
    "                model = Speaking_voice_model\n",
    "                audio_file_path = TakeRealTimeAudio()\n",
    "                #print(\"audio submit\")\n",
    "                #audio_file_path = real.result()\n",
    "                predicted_Speaking_class_name = detect_speaking_file(model, audio_file_path, target_sr=target_sample_rate, gain_factor=2.0)\n",
    "                if predicted_Speaking_class_name == \"Speaking\":\n",
    "                    engine.say(\"YOU ARE SPEAKING TO A PASSANGER PLEASE FOCUS ON THE ROAD\")\n",
    "                    engine.runAndWait()\n",
    "                elif predicted_Speaking_class_name == \"Silent\":\n",
    "                    print(\"silent AND NOT DROWSY SO pass\")\n",
    "           \n",
    "            distraction_flag=0\n",
    "            drowsiness_flag=0\n",
    "            check_flag_dist=1\n",
    "            check_flag_drow=1\n",
    "            print(\"the system will wait for some frames and 30 seconds  till the next detection\")\n",
    "            time. sleep(30) #get the frame every 10 seconds\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames from drowsiness are taken from drowsiness\n",
      "frames from distraction are taken from distraction\n",
      "driver is driving safe but he is drowsy so we need chatbot in order to confirm so\n",
      "Chatgpt in progress\n",
      "Listening ...\n",
      "Stopped\n",
      "Recognizing...\n",
      "User said: no thank you I'm fine I'm awake\n",
      "\n",
      "Thanks for giving me your time\n",
      "the system will wait for some frames and 30 seconds  till the next detection\n",
      "frames from drowsiness are taken from drowsiness\n",
      "frames from distraction are taken from distraction\n",
      "driver is distracted by texting\n",
      "Listening ...\n",
      "Stopped\n",
      "Recognizing...\n",
      "User said: search Google for me please yes please open Google Now\n",
      "\n",
      "I have opened Google for you. Is there anything specific you would like me to search for you? And remember, please keep your focus on the road for your safety.\n",
      "Listening ...\n",
      "Stopped\n",
      "Recognizing...\n",
      "User said: no thank you that's perfect thank you\n",
      "\n",
      "Thanks for giving me your time\n",
      "the system will wait for some frames and 30 seconds  till the next detection\n",
      "frames from drowsiness are taken from drowsiness\n",
      "frames from distraction are taken from distraction\n",
      "Listening...\n",
      "<class 'speech_recognition.audio.AudioData'>\n",
      "Opening wave file....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "c:\\Users\\lenovo\\miniconda3\\envs\\gp\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaking Model in progress...\n",
      "the system will wait for some frames and 30 seconds  till the next detection\n",
      "frames from distraction are taken from distraction\n",
      "frames from drowsiness are taken from drowsiness\n",
      "(0, 0, 3)\n",
      "(0, 0, 3)\n",
      "(0, 0, 3)\n",
      "(0, 0, 3)\n",
      "driver is distracted by Reach Behind\n",
      "the system will wait for some frames and 30 seconds  till the next detection\n",
      "frames from drowsiness are taken from drowsiness\n",
      "frames from distraction are taken from distraction\n",
      "driver is distracted by  Drink\n",
      "the system will wait for some frames and 30 seconds  till the next detection\n",
      "Error: Could not read frame DIST.Error: Could not read frame drowsiness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thread1.start()\n",
    "thread3.start()\n",
    "\n",
    "#print(dist_predicted_class)\n",
    "# In order for the cameras to capture few frames and calculate the weighted average \n",
    "#time. sleep(30) \n",
    "thread2=threading.Thread(target=check_result)\n",
    "thread2.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
